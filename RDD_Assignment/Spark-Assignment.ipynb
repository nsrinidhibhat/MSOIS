{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Create RDDs in three different ways."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.session.SparkSession"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"application\").getOrCreate()\n",
    "type(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['My',\n",
       " 'mission',\n",
       " 'in',\n",
       " 'life',\n",
       " 'is',\n",
       " 'not',\n",
       " 'merely',\n",
       " 'to',\n",
       " 'survive',\n",
       " 'but',\n",
       " 'to',\n",
       " 'thrive',\n",
       " 'and',\n",
       " 'to',\n",
       " 'do',\n",
       " 'so',\n",
       " 'with',\n",
       " 'some',\n",
       " 'passion,some',\n",
       " 'compassion,',\n",
       " 'some',\n",
       " 'humor,',\n",
       " 'and',\n",
       " 'some',\n",
       " 'style.']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#1st method using parallelize\n",
    "text=\"My mission in life is not merely to survive but to thrive and to do so with some passion,some compassion, some humor, and some style.\".split(\" \")\n",
    "words = spark.sparkContext.parallelize(text,2)\n",
    "type(words)\n",
    "words.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.RDD"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#2nd method from data source\n",
    "rdd2=spark.sparkContext.textFile(\"text.txt\")\n",
    "rdd2.collect()\n",
    "type(rdd2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.PipelinedRDD"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#3rd method pipelined RDD\n",
    "rdd3=words.filter(lambda word:word.startswith('s'))\n",
    "rdd3.collect()\n",
    "type(rdd3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Read a text file and count the number of words in the file using RDD operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_count=rdd2.flatMap(lambda s:s.split(\" \"))\n",
    "word_count.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Write a program to find the word frequency in a given file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Nothing', 1),\n",
       " ('chance', 1),\n",
       " ('means', 1),\n",
       " ('of', 3),\n",
       " ('luck.', 1),\n",
       " ('Illness,', 1),\n",
       " ('injury,', 1),\n",
       " ('lost', 1),\n",
       " ('true', 1),\n",
       " ('greatness,', 1),\n",
       " ('occur', 1),\n",
       " ('test', 1),\n",
       " ('Without', 1),\n",
       " ('these', 1),\n",
       " ('tests,', 1),\n",
       " ('may', 1),\n",
       " ('would', 2),\n",
       " ('like', 1),\n",
       " ('straight,', 1),\n",
       " ('flat', 1),\n",
       " ('road', 1),\n",
       " ('nowhere.', 1),\n",
       " ('It', 1),\n",
       " ('safe', 1),\n",
       " ('comfortable,', 1),\n",
       " ('but', 1),\n",
       " ('dull', 1),\n",
       " ('Everything', 1),\n",
       " ('happens', 2),\n",
       " ('for', 1),\n",
       " ('a', 2),\n",
       " ('reason.', 1),\n",
       " ('by', 2),\n",
       " ('or', 1),\n",
       " ('love,', 1),\n",
       " ('moments', 1),\n",
       " ('and', 3),\n",
       " ('sheer', 1),\n",
       " ('stupidity', 1),\n",
       " ('all', 1),\n",
       " ('to', 2),\n",
       " ('the', 1),\n",
       " ('limits', 1),\n",
       " ('your', 1),\n",
       " ('soul.', 1),\n",
       " ('small', 1),\n",
       " ('whatever', 1),\n",
       " ('they', 1),\n",
       " ('be,', 1),\n",
       " ('life', 1),\n",
       " ('be', 2),\n",
       " ('smoothly', 1),\n",
       " ('paved,', 1),\n",
       " ('utterly', 1),\n",
       " ('pointless.', 1)]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info=spark.sparkContext.textFile(\"text.txt\")\n",
    "info=info.flatMap(lambda x:x.split())\n",
    "content_map=info.map(lambda c:(c,1))\n",
    "content_map.reduceByKey(lambda a,b:a+b).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.Write a program to convert all words in a file to uppercase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['EVERYTHING HAPPENS FOR A REASON. NOTHING HAPPENS BY CHANCE OR BY MEANS OF LUCK.',\n",
       " 'ILLNESS, INJURY, LOVE, LOST MOMENTS OF TRUE GREATNESS, AND SHEER STUPIDITY ALL',\n",
       " 'OCCUR TO TEST THE LIMITS OF YOUR SOUL.',\n",
       " 'WITHOUT THESE SMALL TESTS, WHATEVER THEY MAY BE, LIFE WOULD BE LIKE A SMOOTHLY PAVED,',\n",
       " 'STRAIGHT, FLAT ROAD TO NOWHERE.',\n",
       " 'IT WOULD BE SAFE AND COMFORTABLE, BUT DULL AND UTTERLY POINTLESS.']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd2.map(lambda c:c.upper()).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.Write a program to convert all words in a file to lowercase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['everything happens for a reason. nothing happens by chance or by means of luck.',\n",
       " 'illness, injury, love, lost moments of true greatness, and sheer stupidity all',\n",
       " 'occur to test the limits of your soul.',\n",
       " 'without these small tests, whatever they may be, life would be like a smoothly paved,',\n",
       " 'straight, flat road to nowhere.',\n",
       " 'it would be safe and comfortable, but dull and utterly pointless.']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd2.map(lambda c:c.lower()).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.Write a program to capitalize first letter of each words in file (use string capitalize() method)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Everything Happens For A Reason. Nothing Happens By Chance Or By Means Of Luck. Illness, Injury, Love, Lost Moments Of True Greatness, And Sheer Stupidity All Occur To Test The Limits Of Your Soul. Without These Small Tests, Whatever They May Be, Life Would Be Like A Smoothly Paved, Straight, Flat Road To Nowhere. It Would Be Safe And Comfortable, But Dull And Utterly Pointless.'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "capital=rdd2.flatMap(lambda a:a.split(\" \")).map(lambda c:c.capitalize()).collect()\n",
    "\" \".join(capital)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.Find the longest length of word from given set of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'comfortable,'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "longest_word=rdd2.flatMap(lambda x:x.split(\" \"))\n",
    "longest_word.map(lambda nu:(len(nu),nu)).max()[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Map the Registration numbers to corresponding branch. 6000 series BDA, 9000 series HAD, 1000 series MS, 2000 series VLSI, 3000 series ES, 4000 series MSc, 5000 series CC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('BDA', 6027),\n",
       " ('VLSI', 2005),\n",
       " ('VLSI', 2035),\n",
       " ('BDA', 6011),\n",
       " ('HDA', 9007),\n",
       " ('HDA', 9056),\n",
       " ('ES', 3088),\n",
       " ('ES', 3045),\n",
       " ('MSc', 4088),\n",
       " ('MSc', 4065),\n",
       " ('CC', 5077),\n",
       " ('CC', 5066),\n",
       " ('MS', 1001),\n",
       " ('MS', 1002)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "registration_number=[6027,2005,2035,6011,9007,9056,3088,3045,4088,4065,5077,5066,1001,1002]\n",
    "context=spark.sparkContext.parallelize(registration_number,2)\n",
    "classify=context.map(lambda reg:('VLSI',reg) if reg>2000 and reg<3000 \n",
    "        else ('MS',reg) if reg>1000 and reg<2000\n",
    "        else ('ES',reg) if reg>3000 and reg<4000\n",
    "        else ('MSc',reg) if reg>4000 and reg<5000\n",
    "        else ('CC',reg) if reg>5000 and reg<6000\n",
    "        else ('BDA',reg) if reg>6000 and reg<7000\n",
    "        else ('HDA',reg))\n",
    "classify.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#9.Given registration number, generate a key-value pair of Registration Number and Corresponding Branch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('BDA', 6027)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg_no=[6027]\n",
    "info=spark.sparkContext.parallelize(reg_no,2)\n",
    "ans=info.map(lambda reg:('VLSI',reg) if reg>2000 and reg<3000 \n",
    "        else ('MS',reg) if reg>1000 and reg<2000\n",
    "        else ('ES',reg) if reg>3000 and reg<4000\n",
    "        else ('MSc',reg) if reg>4000 and reg<5000\n",
    "        else ('CC',reg) if reg>5000 and reg<6000\n",
    "        else ('BDA',reg) if reg>6000 and reg<7000\n",
    "        else ('HDA',reg))\n",
    "ans.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.\tA text file contains data about citizens of country. \n",
    "Fields(information in file) are Name, dob, Phone, email and state name. \n",
    "Another file contains mapping of state names to state code like Karnataka is codes as KA, \n",
    "TamilNadu as TN, Kerala KL etc. Compress the file will by changing full state name to state code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Ajith', '16-05-1995', '81235', 'kjay.ajith@gmail.com', 'KA'],\n",
       " ['Swathi', '11-02-1996', '98786', 'swathi@gmail.com', 'JK'],\n",
       " ['Sharanya', '31-01-1996', '98675', 'sharanya@gmail.com', 'AP'],\n",
       " ['Chethana', '12-03-1995', '89523', 'chethana@gmail.com', 'TN'],\n",
       " ['Akshith', '04-07-1993', '78234', 'akshith@gmail.com', 'GJ'],\n",
       " ['Amruta', '21-10-1996', '65432', 'amruta@gmail.com', 'WB'],\n",
       " ['Anupam', '05-09-1993', '87474', 'anupam@gmail.com', 'RJ'],\n",
       " ['Chandan', '13-02-1997', '76893', 'chandan@gmail.com', 'PB'],\n",
       " ['Himana', '04-08-1995', '98234', 'himana@gmail.com', 'JK'],\n",
       " ['Varsha', '05-09-1997', '86574', 'varsha@gmail.com', 'MH'],\n",
       " ['Arun', '17-12-1994', '67895', 'arun@gmail.com', 'UP']]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "details=spark.sparkContext.textFile(\"user_details.txt\")\n",
    "code=spark.sparkContext.textFile(\"state_codes.txt\")\n",
    "details_rdd=details.map(lambda x:x.split(\",\")).collect()\n",
    "code_rdd=code.map(lambda y:y.split(\",\")).collect()\n",
    "for i in range(len(details_rdd)):\n",
    "    for j in range(len(code_rdd)):  \n",
    "        if details_rdd[i][4]==code_rdd[j][0]:\n",
    "            details_rdd[i][4]=code_rdd[j][1]\n",
    "details_rdd              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "stateRDD = spark.sparkContext.textFile('state_codes.txt')\n",
    "stateKey = stRDD.map(lambda word: (word.split(',')[0], word.split(',')[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Karnataka': 'KA',\n",
       " 'JammuKashmir': 'JK',\n",
       " 'Gujarat': 'GJ',\n",
       " 'Maharashtra': 'MH',\n",
       " 'Rajasthan': 'RJ',\n",
       " 'Punjab': 'PB',\n",
       " 'AndhraPradesh': 'AP',\n",
       " 'TamilNadu': 'TN',\n",
       " 'UttarPradesh': 'UP',\n",
       " 'WestBengal': 'WB'}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#creating dictionary\n",
    "code_dict = {}\n",
    "for val in stateKey.collect():\n",
    "    code_dict[val[0]] = val[1]\n",
    "    \n",
    "code_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ajith,16-05-1995,81235,kjay.ajith@gmail.com,Karnataka', 'Swathi,11-02-1996,98786,swathi@gmail.com,JammuKashmir', 'Sharanya,31-01-1996,98675,sharanya@gmail.com,AndhraPradesh', 'Chethana,12-03-1995,89523,chethana@gmail.com,TamilNadu', 'Akshith,04-07-1993,78234,akshith@gmail.com,Gujarat', 'Amruta,21-10-1996,65432,amruta@gmail.com,WestBengal', 'Anupam,05-09-1993,87474,anupam@gmail.com,Rajasthan', 'Chandan,13-02-1997,76893,chandan@gmail.com,Punjab', 'Himana,04-08-1995,98234,himana@gmail.com,JammuKashmir', 'Varsha,05-09-1997,86574,varsha@gmail.com,Maharashtra', 'Arun,17-12-1994,67895,arun@gmail.com,UttarPradesh']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Ajith 16-05-1995 81235 kjay.ajith@gmail.com KA',\n",
       " 'Swathi 11-02-1996 98786 swathi@gmail.com JK',\n",
       " 'Sharanya 31-01-1996 98675 sharanya@gmail.com AP',\n",
       " 'Chethana 12-03-1995 89523 chethana@gmail.com TN',\n",
       " 'Akshith 04-07-1993 78234 akshith@gmail.com GJ',\n",
       " 'Amruta 21-10-1996 65432 amruta@gmail.com WB',\n",
       " 'Anupam 05-09-1993 87474 anupam@gmail.com RJ',\n",
       " 'Chandan 13-02-1997 76893 chandan@gmail.com PB',\n",
       " 'Himana 04-08-1995 98234 himana@gmail.com JK',\n",
       " 'Varsha 05-09-1997 86574 varsha@gmail.com MH',\n",
       " 'Arun 17-12-1994 67895 arun@gmail.com UP']"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mapData = spark.sparkContext.broadcast(code_dict)\n",
    "\n",
    "cityRdd = spark.sparkContext.textFile('user_details.txt')\n",
    "print(cityRdd.collect())\n",
    "def abc(state,codes):\n",
    "    splitData = state.split(',')  \n",
    "    print(splitData)\n",
    "    splitData[4] = codes.value.get(splitData[4])\n",
    "    newData = ' '\n",
    "    newData = newData.join(splitData)\n",
    "    \n",
    "    return newData\n",
    "    \n",
    "mapCitizen = cityRdd.map(lambda data: abc(data,mapData))\n",
    "mapCitizen.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''11.Text file contain numbers. Numbers are separated by one white space. \n",
    "There is no order to store the numbers. One line may contain one or more numbers.\n",
    "Find the maximum, minimum, sum and mean of numbers.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "117"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file1=spark.sparkContext.textFile(\"number.txt\")\n",
    "file1_rdd=file1.flatMap(lambda z:z.split(\" \")).map(lambda c:int(c))\n",
    "file1_rdd.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file1_rdd.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1451"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file1_rdd.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51.82142857142856"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file1_rdd.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
